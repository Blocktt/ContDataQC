#Converts a string of USGS site IDs (comma delimited)
#into an array of site IDs for gage data retrieval
USGSsiteParser <- function(siteIDs) {
USGSsiteVector <- unlist(strsplit(siteIDs, split=", "))
return(USGSsiteVector)
}
#Deletes the input csvs and output QC csvs and Word reports from the server after each download
#(actually, after new data are uploaded)
deleteFiles <- function(directory, inputFiles) {
# #Lists the paths and names of the input csvs
csvsInputsToDelete <- substring(list.files(path = directory, pattern = "QC.*csv", full.names = FALSE), 4)
#csvsInputsToDelete <- paste(directory, csvsInputsToDelete, sep="/")
csvsInputsToDelete <- file.path(directory, csvsInputsToDelete)
#Lists all the output csvs and QC Word documents on the server from QCRaw
csvsOutputsToDelete <- list.files(path = directory, pattern = "QC.*csv", full.names = TRUE)
htmlOutputsToDelete <- list.files(path = directory, pattern = ".*html", full.names = TRUE)
pdfOutputsToDelete <- list.files(path = directory, pattern = ".*pdf", full.names = TRUE)
logOutputsToDelete <- list.files(path = directory, pattern = ".*tab", full.names = TRUE)
gageOutputsToDelete <- list.files(path = directory, pattern = ".*Gage.*csv", full.names = TRUE)
#inputsToDelete <- paste(directory, inputFiles, sep="/")
inputsToDelete <- file.path(directory, inputFiles)
#Actually deletes the files
file.remove(csvsOutputsToDelete)
file.remove(htmlOutputsToDelete)
file.remove(pdfOutputsToDelete)
file.remove(logOutputsToDelete)
file.remove(csvsInputsToDelete)
file.remove(gageOutputsToDelete)
}
#Renames the outputs of the Aggregate process.
#Deletes the "append_x" part of the names (csv and html)
#and replaces that with the ending date of the latest file.
renameAggOutput <- function(directory, fileAttribsTable) {
#All files in the working directory
allFiles <- list.files(directory)
#Creates vectors for the input and output files
csvInputs <- vector()
csvOutput <- vector()
htmlOutput <- vector()
#Populates vectors for the input and output files if the input files are
#output from the Aggregate process
if ("DATA_DATA" %in% substr(allFiles,1,9)){
csvInputs <- list.files(directory, pattern = "^DATA_QC.*csv")
csvOutput <- list.files(directory, pattern = "DATA_DATA_QC.*csv")
htmlOutput <- list.files(directory, pattern = "DATA_DATA_QC.*html")
}
#Populates vectors for the input and output files if the input files are
#output from non-Aggregate processes
else{
#Gets the input csvs and output csv and HTML reports
csvInputs <- list.files(directory, pattern = "^QC.*csv")
csvOutput <- list.files(directory, pattern = "DATA_QC.*csv")
htmlOutput <- list.files(directory, pattern = "DATA_QC.*html")
}
#For parsing the file names
myDelim <- "_"
#Creates vector for storing the data types for the input files
data.type.inputs <- vector()
#Extracts the data type from all input files
for (i in 1:length(csvInputs)){
csvInputs.parts <- strsplit(csvInputs[i], myDelim)
data.type.inputs.number <- which(csvInputs.parts[[1]] == fileAttribsTable[1,2]) + 1
data.type.inputs[i] <- csvInputs.parts[[1]][data.type.inputs.number]
}
#Gets the earliest and latest starting and ending dates for all input files
minStartDate <- min(fileAttribsTable[,3])
maxStartDate <- max(fileAttribsTable[,3])
minEndDate <- min(fileAttribsTable[,4])
maxEndDate <- max(fileAttribsTable[,4])
#Changes the output file names if the input files have different data types
#and have the same start and end dates
if(data.type.inputs[1] != data.type.inputs[2]
&& minStartDate == maxStartDate
&& minEndDate == maxEndDate){
#Creates a data.frame for converting the input data types to the output data type
first.data.type <-      c("A",  "A",  "W",  "W",  "G",  "G",  "AW", "AW",  "AG",  "WG",  "G",   "W",   "A")
second.data.type <-     c("W",  "G",  "A",  "G",  "A",  "W",  "AW", "G",   "W",   "A",   "AW",  "AG",  "WG")
combined.data.type <-   c("AW", "AG", "AW", "WG", "AG", "WG", "AW", "AWG", "AWG", "AWG", "AWG", "AWG", "AWG")
data.type.conversion <- data.frame(first.data.type, second.data.type, combined.data.type)
#Finds the rows in the conversion data.frame which have that date type
type1 <- which(data.type.conversion[,1]==data.type.inputs[1])
type2 <- which(data.type.conversion[,2]==data.type.inputs[2])
#Finds the one row on the conversion data.frame that corresponds to those data types
output.type <- as.character(data.type.conversion[intersect(type1, type2), 3])
#Changes the data type in the output csv to the correct datatype
csvOutput.data.type <- gsub(paste("_", data.type.inputs[1], "_", sep=""),
paste("_", output.type, "_", sep=""),
csvOutput)
#Sometimes the Aggregate process uses the second data type in its output name.
#This captures that eventuality.
csvOutput.data.type <- gsub(paste("_", data.type.inputs[2], "_", sep=""),
paste("_", output.type, "_", sep=""),
csvOutput.data.type)
#Removes the "_append_x" from the output csv name
csvOutput.data.type <- gsub(paste("_Append_", length(csvInputs), sep=""),
paste("", sep=""),
csvOutput.data.type)
#Renames the csv
file.rename(file.path(".", "data", csvOutput)
, file.path(".", "data", csvOutput.data.type))
#Changes the data type in the output html to the correct datatype
htmlOutput.data.type <- gsub(paste("_", data.type.inputs[1], "_", sep=""),
paste("_", output.type, "_", sep=""),
htmlOutput)
#Sometimes the Aggregate process uses the second data type in its output name.
#This captures that eventuality.
htmlOutput.data.type <- gsub(paste("_", data.type.inputs[2], "_", sep=""),
paste("_", output.type, "_", sep=""),
htmlOutput.data.type)
#Removes the "_append_x" from the output html name
htmlOutput.data.type <- gsub(paste("_Append_", length(csvInputs), sep=""),
paste("", sep=""),
htmlOutput.data.type)
#Removes the "_Report_Aggregate" from the output html name
htmlOutput.data.type <- gsub(paste("_Report_Aggregate", sep=""),
paste("", sep=""),
htmlOutput.data.type)
#Renames the html output
file.rename(file.path(".", "data", htmlOutput)
, file.path(".", "data", htmlOutput.data.type))
}
#Changes the output file names if the input files are from different time periods
else {
#Identifies the minimum and maximum dates in all input files
minDate <- min(fileAttribsTable[,3])
minDate <- gsub("-", "", minDate)
maxDate <- max(fileAttribsTable[,4])
maxDate <- gsub("-", "", maxDate)
#The number of csv files input to the Aggregate step
numFiles <- length(csvInputs)
#Determines how many characters should be removed from end of csv output name
#(through the (inaccurate) second date in the name, which is not the latest
#date of measurement)
csvCharsToRemove <- 8+1+8+1+6+1+nchar(numFiles)+4
# date min (8) + sep (1) + date max (8) + sep (1) + append (6) + NumFiles + .ext (4)
#Removes the specified number of characters
csvNewName <- substr(csvOutput, 0, nchar(csvOutput)-csvCharsToRemove)
#Adds the last recorded date to the truncated file name
csvNewName <- paste(csvNewName, minDate, "_", maxDate, ".csv", sep="")
#Replaces the old file name with the new one
file.rename(file.path(".", "data", csvOutput)
, file.path(".", "data", csvNewName))
#Same as above but for the HTML reports
htmlCharsToRemove <- 8+1+8+1+6+1+nchar(numFiles)+1+6+1+8+6
# date min (8) + sep (1) + date max (8) + sep (1) + append (6) + NumFiles +
# sep (1) + Report (6) + sep (1) + Aggregate (8) + .ext (6?)
htmlNewName <- substr(htmlOutput, 0, nchar(htmlOutput)-htmlCharsToRemove)
htmlNewName <- paste(htmlNewName, minDate, "_", maxDate, ".html", sep="")
file.rename(file.path(".", "data", htmlOutput)
, file.path(".", "data", htmlNewName))
}## IF ~ END
}## FUNCTION ~renameAggOutput ~ END
######Not currently using
#Function to parse out the station ID, data type, and starting and ending
#dates from the input file name.
#This is copied from the filename parser used by the QC script itself.
#It exports a data.frame in which each property is its own column.
#It extracts file information differently based on what process is being run
nameParse <- function(strFile, process) {
#Sets up the parsing
myDelim <- "_"
strFile.Base <- substr(strFile,1,nchar(strFile)-nchar(".csv"))
strFile.parts <- strsplit(strFile.Base, myDelim)
#Parsing for the Aggregate step. Files being aggregated have "QC_" prepended.
if (process == "Aggregate") {
strFile.SiteID     <- strFile.parts[[1]][2]
strFile.DataType   <- strFile.parts[[1]][3]
# Convert Data Type to proper case
strFile.DataType <- paste(toupper(substring(strFile.DataType,1,1)),tolower(substring(strFile.DataType,2,nchar(strFile.DataType))),sep="")
strFile.Date.Start <- as.Date(strFile.parts[[1]][4],"%Y%m%d")
strFile.Date.End   <- as.Date(strFile.parts[[1]][5],"%Y%m%d")
}
#Parsing for the SummaryStats step. Files being aggregated have "DATA_" prepended.
else if (process == "SummaryStats") {
strFile.SiteID     <- strFile.parts[[1]][3]
strFile.DataType   <- strFile.parts[[1]][4]
# Convert Data Type to proper case
strFile.DataType <- paste(toupper(substring(strFile.DataType,1,1)),tolower(substring(strFile.DataType,2,nchar(strFile.DataType))),sep="")
strFile.Date.Start <- as.Date(strFile.parts[[1]][5],"%Y%m%d")
strFile.Date.End   <- as.Date(strFile.parts[[1]][6],"%Y%m%d")
}
#Parsing for the QCRaw or GetgageData steps.
else {
strFile.SiteID     <- strFile.parts[[1]][1]
strFile.DataType   <- strFile.parts[[1]][2]
# Convert Data Type to proper case
strFile.DataType <- paste(toupper(substring(strFile.DataType,1,1)),tolower(substring(strFile.DataType,2,nchar(strFile.DataType))),sep="")
strFile.Date.Start <- as.Date(strFile.parts[[1]][3],"%Y%m%d")
strFile.Date.End   <- as.Date(strFile.parts[[1]][4],"%Y%m%d")
}
siteDF <- data.frame(strFile.SiteID, strFile.DataType, strFile.Date.Start, strFile.Date.End)
return(siteDF)
}
USGSsiteParser("x")
runApp('inst/shiny-examples/ContDataQC')
USGSsiteVector
ContDataQC(
myData.Operation        <- "GetGageData",
myData.SiteID           <- USGSsiteVector[i],
myData.Type             <- "Gage",
myData.DateRange.Start  <- input$startDate,
myData.DateRange.End    <- input$endDate,
myDir.import            <- "",
myDir.export            <- file.path(".", "data"),
fun.myReport.Dir        <- ""
)
getwd()
setwd("C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub/ContDataQC/inst/shiny-examples/ContDataQC")
?textInput
runApp()
runApp()
getwd()
dir(file.path(".", "data")
)
dir(file.path(".", "data"), full.names=FALSE, pattern=".*Gage.*csv")
dir(file.path("data"), full.names=FALSE, pattern=".*Gage.*csv")
shiny::runApp('inst/shiny-examples/ContDataQC')
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
devtools::install_github("leppott/ContDataQC", force = TRUE)
usethis::use_github_actions()
badger::badge_lifecycle("stable", "green")
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
usethis::use_github_action("pkgdown")
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
usethis::use_github_action("test-coverage")
badger::badge_codecov(pkg_GH)
pkg_GH <- "leppott/ContDataQC"
badger::badge_codefactor(pkg_GH)
# code testing coverage
badger::badge_codecov(pkg_GH)
usethis::use_github_action("test-coverage")
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
goodpractice::gp()
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
library(ContDataQC)
usethis::use_github_action("pkgdown")
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
usethis::use_github_action("pkgdown")
# Set working directory
myLibrary <- "ContDataQC"
dir_base <- "C:/Users/Erik.Leppo/OneDrive - Tetra Tech, Inc/MyDocs_OneDrive/GitHub"
setwd(file.path(dir_base, myLibrary))
#
# NEWS
# Render then Copy NEWS so picked up in help
rmarkdown::render("NEWS.rmd", "all")
file.copy("NEWS.md", "NEWS", overwrite = TRUE)
file.remove("NEWS.html")
file.remove("NEWS.md")
#
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary, build_vignettes = TRUE, quick = FALSE, reload = TRUE)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(file.path(dir_base, myLibrary))
