#   myMsg <- "Function can only handle 1 Data Type."
#   stop(myMsg)
# }
#
# Convert Data Type to proper case
# fun.myData.Type <- paste(toupper(substring(fun.myData.Type,1,1)),tolower(substring(fun.myData.Type,2,nchar(fun.myData.Type))),sep="")
#
# data directories
# myDir.data.import <- paste(fun.myDir.BASE,ifelse(fun.myDir.SUB.import=="","",paste("/",fun.myDir.SUB.import,sep="")),sep="")
# myDir.data.export <- paste(fun.myDir.BASE,ifelse(fun.myDir.SUB.export=="","",paste("/",fun.myDir.SUB.export,sep="")),sep="")
myDir.data.import <- fun.myDir.import
myDir.data.export <- fun.myDir.export
#
myDate <- format(Sys.Date(),"%Y%m%d")
myTime <- format(Sys.time(),"%H%M%S")
# Check on number of files
files2process <- fun.myFile
# Define Counters for the Loop
intCounter <- 0
intCounter.Stop <- length(files2process)
intItems.Total <- intCounter.Stop
print(paste("Total files to process = ",intItems.Total,sep=""))
flush.console()
myItems.Complete  <- 0
myItems.Skipped   <- 0
# Create Log file
##  List of all items (files)
myItems.ALL <- as.vector(unique(files2process))
# create log file for processing results of items
#myItems.Log <- data.frame(cbind(myItems.ALL,NA),stringsAsFactors=FALSE)
myItems.Log <- data.frame(ItemID=1:intItems.Total,Status=NA,ItemName=myItems.ALL)
# Start Time (used to determine run time at end)
myTime.Start <- Sys.time()
#
# B.0. Increase the Counter
intCounter <- intCounter+1
#
# B.1.0. File Name, Define
strFile <- files2process[intCounter]
strFile.Base <- substr(strFile,1,nchar(strFile)-nchar(".csv"))
#
#QC, make sure file exists
if(strFile %in% list.files(path=myDir.data.import)==FALSE) {##IF.file.START
#
print("ERROR; no such file exits.  Cannot generate summary statistics.")
print(paste("PATH = ",myDir.data.import,sep=""))
print(paste("FILE = ",strFile,sep=""))
flush.console()
#
# maybe print similar files
#
stop("Bad file.")
#
}##IF.file.END
# B.2.1. Check File Size
#if(file.info(paste(myDir.data.import,"/",strFile,sep=""))$size==0){
if(file.info(file.path(myDir.data.import,strFile))$size==0){
# inform user of progress and update LOG
myMsg <- "SKIPPED (file blank)"
myItems.Skipped <- myItems.Skipped + 1
myItems.Log[intCounter,2] <- myMsg
fun.write.log(myItems.Log,myDate,myTime)
fun.Msg.Status(myMsg, intCounter, intItems.Total, strFile)
flush.console()
# go to next Item
next
}
#import the file
data.import <- read.csv(file.path(myDir.data.import,strFile),as.is=TRUE,na.strings="")
#
# QC required fields: SiteID & (DateTime | (Date & Time))
#fun.QC.ReqFlds(names(data.import),paste(myDir.data.import,strFile,sep="/"))
fun.QC.ReqFlds(names(data.import),file.path(myDir.data.import,strFile))
#
# QC date and time
# accessing files with Excel can change formats
# 20170116, EWL
data.import <- fun.QC.datetime(data.import)
myNames.Fields.TimePeriods <- c(ContData.env$myName.Yr, ContData.env$myName.YrMo, ContData.env$myName.MoDa, ContData.env$myName.Mo
, ContData.env$myName.JuDa, ContData.env$myName.Season, ContData.env$myName.YrSeason)
# add time period fields
data.import[,ContData.env$myName.Yr]   <- format(as.Date(data.import[,ContData.env$myName.Date]),format="%Y")
data.import[,ContData.env$myName.Mo]   <- format(as.Date(data.import[,ContData.env$myName.Date]),format="%m")
head(data.gage)
str(data.gage)
data.gage[,ContData.env$myName.Date] <- as.Date(data.gage[,ContData.env$myName.DateTime])
head(data.gage)
data.gage[,ContData.env$myName.Time] <- as.POSIXct(data.gage[,ContData.env$myName.DateTime], format="%H:%M:%S")
head(data.gage)
data.gage[,ContData.env$myName.Time] <-  strftime(data.gage[,ContData.env$myName.DateTime], format="%H:%M:%S")
head(data.gage)
write.csv(data.gage, myFile, row.names=FALSE)
myData.Operation <- "SummaryStats"
myDir.import <- getwd()
myDir.export <- getwd()
ContDataQC(myData.Operation, fun.myDir.import=myDir.import
, fun.myDir.export=myDir.export, fun.myFile=myFile)
myData.Operation <- "SummaryStats"
myDir.import <- getwd()
myDir.export <- getwd()
ContDataQC(myData.Operation, fun.myDir.import=myDir.import
, fun.myDir.export=myDir.export, fun.myFile=myFile)
head(data.gage)
ContDataQC(myData.Operation, fun.myDir.import=myDir.import
, fun.myDir.export=myDir.export, fun.myFile=myFile)
write.csv(data.gage, myFile, row.names=FALSE)
ContDataQC(myData.Operation, fun.myDir.import=myDir.import
, fun.myDir.export=myDir.export, fun.myFile=myFile)
?ContDataQC
# Get Gage Data via the dataRetrieval package from USGS 01187300 2013
data.gage <- dataRetrieval::readNWISdv("03238500", "00060", "1974-10-01", "1975-09-30")
head(data.gage)
# flow data
data.Q <- data.gage[,4]
# remove zeros
data.Q[data.Q==0] <- NA
RBIcalc(data.Q)
RBIcalc(data.Q)
x <- rnorm(1:100)
x
RBIcalc(x)
wd <- file.path("P:","Current","USEPA","NCEA","Climate","DataInfrastructure","QCTesting20171127","Charlies")
wd <- file.path("P:","Current","USEPA","NCEA","Climate","DataInfrastructure","QCTesting20171127","Charlies","Data4_Stats")
file <- "DV_QC_B0997_AW_20150305_20151231_Sensor.Depth.ft.csv"
df <- file.path(wd,file)
head(df)
df <- read.csv(file.path(wd,file))
file <- "DV_QC_Charlies_Aw_20130312_20130410_Sensor.Depth.ft.csv"
df <- read.csv(file.path(wd,file))
head(df)
RBIcalc(df$mean)
# 1.Define Data
#
# 1.1. Get USGS data
# code from StreamThermal T_frequency example
ExUSGSStreamTemp<-readNWISdv("01382310","00010","2011-01-01","2011-12-31"
,c("00001","00002","00003"))
sitedata<-subset(ExUSGSStreamTemp, select=c("site_no","Date"
,"X_00010_00001","X_00010_00002","X_00010_00003"))
names(sitedata)<-c("siteID","Date","MaxT","MinT","MeanT")
# 1.2. Use ContDataQC SummaryStats Data
myFile <- "STATS_test2_Aw_20130101_20141231_Water.Temp.C.csv"
myDir <- "Data4_Stats"
myData <- read.csv(file.path(getwd(),myDir,myFile), stringsAsFactors=FALSE)
getwd()
# Subset
Col.Keep <- c("SiteID", "TimeValue", "max", "min", "mean")
sitedata <- myData[myData[,"TimeFrame"]=="day",Col.Keep]
Names.ST <- c("SiteID", "Date", "MaxT", "MinT", "MeanT")
names(sitedata) <- Names.ST
# Convert date column to date type
sitedata[,"Date"] <- as.Date(sitedata[,"Date"])
sitedata <- Export.StreamThermal(myData)
sitedata <- Export.StreamThermal(sitedata)
?ContDataQC
# Examples of each operation
# Parameters
Selection.Operation <- c("GetGageData","QCRaw", "Aggregate", "SummaryStats")
Selection.Type      <- c("Air","Water","AW","Gage","AWG","AG","WG")
Selection.SUB <- c("Data1_RAW","Data2_QC","Data3_Aggregated","Data4_Stats")
myDir.BASE <- getwd()
# Create data directories
myDir.create <- paste0("./",Selection.SUB[1])
ifelse(dir.exists(myDir.create)==FALSE,dir.create(myDir.create),"Directory already exists")
myDir.create <- paste0("./",Selection.SUB[2])
ifelse(dir.exists(myDir.create)==FALSE,dir.create(myDir.create),"Directory already exists")
myDir.create <- paste0("./",Selection.SUB[3])
ifelse(dir.exists(myDir.create)==FALSE,dir.create(myDir.create),"Directory already exists")
myDir.create <- paste0("./",Selection.SUB[4])
ifelse(dir.exists(myDir.create)==FALSE,dir.create(myDir.create),"Directory already exists")
# Save example data (assumes directory ./Data1_RAW/ exists)
myData <- data_raw_test2_AW_20130426_20130725
write.csv(myData,paste0("./",Selection.SUB[1],"/test2_AW_20130426_20130725.csv"))
myData <- data_raw_test2_AW_20130725_20131015
write.csv(myData,paste0("./",Selection.SUB[1],"/test2_AW_20130725_20131015.csv"))
myData <- data_raw_test2_AW_20140901_20140930
write.csv(myData,paste0("./",Selection.SUB[1],"/test2_AW_20140901_20140930.csv"))
myData <- data_raw_test4_AW_20160418_20160726
write.csv(myData,paste0("./",Selection.SUB[1],"/test4_AW_20160418_20160726.csv"))
myFile <- "config.TZ.Central.R"
file.copy(file.path(path.package("ContDataQC"),"extdata",myFile)
,file.path(getwd(),Selection.SUB[1],myFile))
# QC Raw Data
myData.Operation <- "QCRaw" #Selection.Operation[2]
myData.SiteID    <- "test2"
myData.Type      <- Selection.Type[3] #"AW"
myData.DateRange.Start  <- "2013-01-01"
myData.DateRange.End    <- "2014-12-31"
myDir.import <- file.path(myDir.BASE,Selection.SUB[1]) #"Data1_RAW"
myDir.export <- file.path(myDir.BASE,Selection.SUB[2]) #"Data2_QC"
myReport.format <- "docx"
ContDataQC(myData.Operation, myData.SiteID, myData.Type, myData.DateRange.Start
, myData.DateRange.End, myDir.import, myDir.export
, fun.myReport.format=myReport.format)
# Aggregate Data
myData.Operation <- "Aggregate" #Selection.Operation[3]
myData.SiteID    <- "test2"
myData.Type      <- Selection.Type[3] #"AW"
myData.DateRange.Start  <- "2013-01-01"
myData.DateRange.End    <- "2014-12-31"
myDir.import <- file.path(myDir.BASE,Selection.SUB[2]) #"Data2_QC"
myDir.export <- file.path(myDir.BASE,Selection.SUB[3]) #"Data3_Aggregated"
#Leave off myReport.format and get default (docx).
ContDataQC(myData.Operation, myData.SiteID, myData.Type, myData.DateRange.Start
, myData.DateRange.End, myDir.import, myDir.export)
# Summary Stats
myData.Operation <- "SummaryStats" #Selection.Operation[4]
myData.SiteID    <- "test2"
myData.Type      <- Selection.Type[3] #"AW"
myData.DateRange.Start  <- "2013-01-01"
myData.DateRange.End    <- "2014-12-31"
myDir.import <- file.path(myDir.BASE,Selection.SUB[3]) #"Data3_Aggregated"
myDir.export <- file.path(myDir.BASE,Selection.SUB[4]) #"Data4_Stats"
#Leave off myReport.format and get default (docx).
ContDataQC(myData.Operation, myData.SiteID, myData.Type, myData.DateRange.Start
, myData.DateRange.End, myDir.import, myDir.export)
# 1.2. Use ContDataQC SummaryStats Data
myFile <- "STATS_test2_Aw_20130101_20141231_Water.Temp.C.csv"
myDir <- "Data4_Stats"
myData <- read.csv(file.path(getwd(),myDir,myFile), stringsAsFactors=FALSE)
# Subset
Col.Keep <- c("SiteID", "TimeValue", "max", "min", "mean")
sitedata <- myData[myData[,"TimeFrame"]=="day",Col.Keep]
Names.ST <- c("SiteID", "Date", "MaxT", "MinT", "MeanT")
names(sitedata) <- Names.ST
# Convert date column to date type
sitedata[,"Date"] <- as.Date(sitedata[,"Date"])
#
# 1.3. Use user data that has been QCed
myData <- DATA_period_test2_Aw_20130101_20141231
sitedata <- Export.StreamThermal(myData)
require(StreamThermal)
devtools::install_github("tsangyp/StreamThermal")
require(StreamThermal)
# StreamThermal
(ST.freq <- T_frequency(sitedata))
(ST.mag  <- T_magnitude(sitedata))
(ST.roc  <- T_rateofchange(sitedata))
(ST.tim  <- T_timing(sitedata))
ST.freq
write.csv(ST.freq,"ST.freq.csv")
?T_frequency
# 1.A. Use ContDataQC and Save (~1min for download)
myData.Operation    <- "GetGageData" #Selection.Operation[1]
myData.SiteID       <- "01187300" # Hubbard River near West Hartland, CT
myData.Type         <- "Gage"
myData.DateRange.Start  <- "2015-01-01"
myData.DateRange.End    <- "2016-12-31"
myDir.import <- getwd()
myDir.export <- getwd()
ContDataQC(myData.Operation, myData.SiteID, myData.Type
, myData.DateRange.Start, myData.DateRange.End
, myDir.import, myDir.export)
# 1.B. Use saved data
myFile <- "01187300_Gage_20150101_20161231.csv"
myCol.DateTime="Date.Time"
myCol.Discharge <- "Discharge.ft3.s"
# 2. Prep Data
myData.IHA <- Export.IHA(myFile
, fun.myCol.DateTime = myCol.DateTime
, fun.myCol.Parameter = myCol.Discharge
)
getwd()
# User info
SiteID <- myData.SiteID
Notes.User <- "Erik.Leppo@tetratech.com"
devtools::install_github("jasonelaw/IHA")
install.packages("XLConnect")
require(IHA)
require(XLConnect)
# IHA
myYr <- "calendar" # "water" or "calendar"
# IHA Metrics
## IHA parameters group 1; Magnitude of monthly water conditions
Analysis.Group.1 <- group1(myData.IHA, year=myYr)
## IHA parameters group 2: Magnitude of monthly water condition and include 12 parameters
Analysis.Group.2 <- group2(myData.IHA, year=myYr)
Analysis.Group.3 <- group3(myData.IHA, year=myYr)
## IHA parameters group 4; Frequency and duration of high and low pulses
# defaults to 25th and 75th percentiles
Analysis.Group.4 <- group4(myData.IHA, year=myYr)
## IHA parameters group 5; Rate and frequency of water condition changes
Analysis.Group.5 <- group5(myData.IHA, year=myYr)
# Save Results to Excel (each group on its own worksheet)
Group.Desc <- c("Magnitude of monthly water conditions"
,"Magnitude of monthly water condition and include 12 parameters"
,"Timing of annual extreme water conditions"
,"Frequency and duration of high and low pulses"
,"Rate and frequency of water condition changes")
df.Groups <- as.data.frame(cbind(paste0("Group",1:5),Group.Desc))
#
myDate <- format(Sys.Date(),"%Y%m%d")
myTime <- format(Sys.time(),"%H%M%S")
# Notes section (add min/max dates)
Notes.Names <- c("Dataset (SiteID)","IHA.Year","Analysis.Date (YYYYMMDD)"
,"Analysis.Time (HHMMSS)","Analysis.User")
Notes.Data <- c(SiteID, myYr, myDate, myTime, Notes.User)
df.Notes <- as.data.frame(cbind(Notes.Names,Notes.Data))
Notes.Summary <- summary(myData.IHA)
# Open/Create file
myFile.XLSX <- paste("IHA", SiteID, myYr, myDate, myTime, "xlsx", sep=".")
wb <- loadWorkbook(myFile.XLSX, create = TRUE) # load workbook, create if not existing
# create sheets
createSheet(wb, name = "NOTES")
createSheet(wb, name = "Group1")
createSheet(wb, name = "Group2")
createSheet(wb, name = "Group3")
createSheet(wb, name = "Group4")
createSheet(wb, name = "Group5")
# write to worksheet
writeWorksheet(wb, df.Notes, sheet = "NOTES", startRow=1)
writeWorksheet(wb, Notes.Summary, sheet = "NOTES", startRow=10)
writeWorksheet(wb, df.Groups, sheet="NOTES", startRow=25)
writeWorksheet(wb, Analysis.Group.1, sheet = "Group1")
writeWorksheet(wb, Analysis.Group.2, sheet = "Group2")
writeWorksheet(wb, Analysis.Group.3, sheet = "Group3")
writeWorksheet(wb, Analysis.Group.4, sheet = "Group4")
writeWorksheet(wb, Analysis.Group.5, sheet = "Group5")
# save workbook
saveWorkbook(wb, myFile.XLSX)
Analysis.Group.1
Analysis.Group.2
# Variant 1 (with File)
# Load Data
myFile <- "CDF_WaterTemp_2014_MA.csv"
# example file from ContDataQC library files
myDir.input <- file.path(path.package("ContDataQC"),"extdata")
myDir.output <- getwd()
# X Label
myXlab <- "Temperature, Water (deg C)"
# Run the Function
CompSiteCDF(file.input=myFile, dir.input=myDir.input
, dir.output=myDir.output, ParamName.xlab=myXlab)
?ContDataQC
# Get Gage Data
myData.Operation    <- "GetGageData" #Selection.Operation[1]
myData.SiteID       <- "01187300" # Hubbard River near West Hartland, CT
myData.Type         <- Selection.Type[4] #"Gage"
myData.DateRange.Start  <- "2013-01-01"
myData.DateRange.End    <- "2014-12-31"
myDir.import <- ""
myDir.export <- file.path(myDir.BASE,Selection.SUB[1])
ContDataQC(myData.Operation, myData.SiteID, myData.Type, myData.DateRange.Start
, myData.DateRange.End, myDir.import, myDir.export)
# Library Name
myLibrary <- "ContDataQC"
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
setwd(paste0("./",myLibrary))
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(paste0("./",myLibrary))
# Library Name
myLibrary <- "ContDataQC"
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
setwd(paste0("./",myLibrary))
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(paste0("./",myLibrary))
library(ContDataQC)
# Library Name
myLibrary <- "ContDataQC"
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
setwd(paste0("./",myLibrary))
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(paste0("./",myLibrary))
ContDataQC
library(ContDataQC)
?ContDataQC
# Library Name
myLibrary <- "ContDataQC"
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
setwd(paste0("./",myLibrary))
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(paste0("./",myLibrary))
library(ContDataQC)
#?CompSiteCDF
??ContDataQC
# Get Data, e.g., USGS gage data
# Get Gage Data via the dataRetrieval package from USGS 01187300 2013 (~4 seconds)
data.gage <- dataRetrieval::readNWISuv("01187300", "00060", "2013-01-01", "2014-12-31")
head(data.gage)
# Rename fields
myNames <- c("Agency", "SiteID", "Date.Time", "Discharge.ft3.s", "Code", "TZ")
names(data.gage) <- myNames
# Add Date and Time
data.gage[,"Date"] <- as.Date(data.gage[,"Date.Time"])
data.gage[,"Time"] <-  strftime(data.gage[,"Date.Time"], format="%H:%M:%S")
# Add "flag" fields that are added by QC function.
Names.Flags <- paste0("Flag.",c("Date.Time", "Discharge.ft3.s")
data.gage[,Names.Flags] <- "P"
paste0("Flag.",c("Date.Time", "Discharge.ft3.s")
)
data.gage[,Names.Flags] <- "P"
Names.Flags <- paste0("Flag.",c("Date.Time", "Discharge.ft3.s"))
data.gage[,Names.Flags] <- "P"
# Save File
myFile <- "01187300_Gage_20130101_20141231.csv"
write.csv(data.gage, myFile, row.names=FALSE)
# Run Stats (File)
myData.Operation <- "SummaryStats"
myDir.import <- getwd()
myDir.export <- getwd()
ContDataQC(myData.Operation, fun.myDir.import=myDir.import
, fun.myDir.export=myDir.export, fun.myFile=myFile)
# generate Vignette
library(ContDataQC)
library(devtools)
devtools::build_vignettes()
# create vignette folder and default file
#devtools::use_vignette("ContDataQC_Vignette")
# Library Name
myLibrary <- "ContDataQC"
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
setwd(paste0("./",myLibrary))
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(paste0("./",myLibrary))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Library Name
myLibrary <- "ContDataQC"
# Load Library
library(devtools)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create Package
# create(myLibrary)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Document, Install, and Reload Library
## Generate Documentation
setwd(paste0("./",myLibrary))
devtools::document()
## Install New Package (locally)
setwd("..") # return to root directory first
devtools::install(myLibrary)
## Reload library
library(myLibrary,character.only = TRUE)
# change wd back to package
setwd(paste0("./",myLibrary))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
